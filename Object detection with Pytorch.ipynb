{"cells":[{"metadata":{"colab_type":"text","id":"MjBeTMLJQJuQ"},"cell_type":"markdown","source":"# Instance Segmentation with Mask Region Based Convolutional Neural Network."},{"metadata":{},"cell_type":"markdown","source":"You should have seen in our previous posts that Convolutional Neural Network is the state of the art for any computer vision task like\n\n- [Image classification](https://www.learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/)\n\n- [Semantic Segmentation](https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/)\n\n- [Object Detection](https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch/)\n\nIn this notebook we will look at another computer vision application called Instance Segmentation. "},{"metadata":{"colab_type":"text","id":"POmWpk8Qy8em"},"cell_type":"markdown","source":"# Instance Segmentation\n  Instance Segmentation is a combination of 2 problems\n  - Object Detection\n  - Semantic Segmentation\n  \n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRmp4t9zSQ9DWyTuWv2OwK4Eq4ydaBbvl1FiaslP_y0etJ89n-5)\n  \nA kind of network called Mask RCNN is the state of the art in Instance Segmentation. Mask RCNN uses 2 type of networks one like Faster RCNN for Object Detection and another fully convolutional network for Semantic Segmentation. The first model will get the bounding box and classify it and the second model is used on each of the region of interest for semantic segmentation. \n  \n![](https://cdn-images-1.medium.com/max/1600/1*IWWOPIYLqqF9i_gXPmBk3g.png)\nImage Source: [Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf)"},{"metadata":{"colab_type":"text","id":"PIWYe-tJyu2t"},"cell_type":"markdown","source":"# Instance Segmentation in PyTorch\n\nIf you want to learn more about all of these models and many more application and concepts of Deep Learning and Computer Vision indetail,  check out the official [Deep Learning and Computer Vision courses](https://opencv.org/courses/) by OpenCV.org.\n\n  All the pretrained models in pytorch can be found in [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)"},{"metadata":{},"cell_type":"markdown","source":"\n\nThe input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes.\n\n\nDuring inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows:\n\n- boxes (Tensor[N, 4]): the predicted boxes in [x0, y0, x1, y1] format, with values between 0 and H and 0 and W\n\n- labels (Tensor[N]): the predicted labels for each image\n\n-  scores (Tensor[N]): the scores or each prediction\n\n-  masks (Tensor[N, H, W]): the predicted masks for each instance, in 0-1 range. In order to obtain the final segmentation masks, the soft masks can be thresholded, generally with a value of 0.5 (mask >= 0.5)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import models","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"CLEfxNKK1OC9","trusted":true},"cell_type":"code","source":"# import necessary libraries\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision.transforms as T\nimport torchvision\nimport torch\nimport numpy as np\nimport cv2\nimport random\n\n# get the pretrained model from torchvision.models\n# Note: pretrained=True will get the pretrained weights for the model.\n# model.eval() to use the model for inference\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('../input/caltech256/256_objectcategories/256_ObjectCategories/072.fire-truck/072_0076.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_colour_masks(image):\n  \"\"\"\n  random_colour_masks\n    parameters:\n      - image - predicted masks\n    method:\n      - the masks of each predicted object is given random colour for visualization\n  \"\"\"\n  colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n  r = np.zeros_like(image).astype(np.uint8)\n  g = np.zeros_like(image).astype(np.uint8)\n  b = np.zeros_like(image).astype(np.uint8)\n  r[image == 1], g[image == 1], b[image == 1] = colours[random.randrange(0,10)]\n  coloured_mask = np.stack([r, g, b], axis=2)\n  return coloured_mask\n\nCOCO_INSTANCE_CATEGORY_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\ndef get_prediction(img_path, threshold):\n  \"\"\"\n  get_prediction\n    parameters:\n      - img_path - path of the input image\n    method:\n      - Image is obtained from the image path\n      - the image is converted to image tensor using PyTorch's Transforms\n      - image is passed through the model to get the predictions\n      - masks, classes and bounding boxes are obtained from the model and soft masks are made binary(0 or 1) on masks\n        ie: eg. segment of cat is made 1 and rest of the image is made 0\n    \n  \"\"\"\n  img = Image.open(img_path)\n  transform = T.Compose([T.ToTensor()])\n  img = transform(img)\n  pred = model([img])\n  pred_score = list(pred[0]['scores'].detach().numpy())\n  pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n  masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n  masks = masks[:pred_t+1]\n  pred_boxes = pred_boxes[:pred_t+1]\n  pred_class = pred_class[:pred_t+1]\n  return masks, pred_boxes, pred_class\n\n\ndef instance_segmentation_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n  \"\"\"\n  instance_segmentation_api\n    parameters:\n      - img_path - path to input image\n    method:\n      - prediction is obtained by get_prediction\n      - each mask is given random color\n      - each mask is added to the image in the ration 1:0.8 with opencv\n      - final output is displayed\n  \"\"\"\n  masks, boxes, pred_cls = get_prediction(img_path, threshold)\n  img = cv2.imread(img_path)\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  for i in range(len(masks)):\n    rgb_mask = random_colour_masks(masks[i])\n    img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n    cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n  plt.figure(figsize=(20,30))\n  plt.imshow(img)\n  plt.xticks([])\n  plt.yticks([])\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torchvision.__version__","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"KCuzRQhm12Xk","outputId":"a699ff6d-b786-4102-8081-30daee5a4d70","trusted":true},"cell_type":"code","source":"colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n\na,b,c=colours[0]\n","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"ohTNwXpz2RfI","outputId":"4daf8abf-3031-41db-f63c-4c0c4a5d25a6","trusted":true},"cell_type":"code","source":"import random\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"89flam1cPf43"},"cell_type":"markdown","source":"## Example to understand how masks are added to the input image"},{"metadata":{"colab":{},"colab_type":"code","id":"_FQ_DW7RNRgf","trusted":true},"cell_type":"code","source":"img = Image.open('../input/caltech256/256_objectcategories/256_ObjectCategories/072.fire-truck/072_0076.jpg')\ntransform = T.Compose([T.ToTensor()])\nimg = transform(img)\npred = model([img])","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"colab_type":"code","id":"v2tNoXmeNSRk","outputId":"8c903503-f7ed-4547-8c1c-61cf5255da37","trusted":true},"cell_type":"code","source":"pred[0]","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"iTL-OFlWNSWq","outputId":"2c9f0b31-b7ab-4f33-82f8-ddefe2599d0c","trusted":true},"cell_type":"code","source":"masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\nmasks.shape","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"colab_type":"code","id":"kXEKjA_fNSZg","outputId":"7e74f1b8-99b9-4895-ca11-3154c7bce1e3","trusted":true},"cell_type":"code","source":"plt.imshow(masks[0], cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"colab_type":"code","id":"KQgEj6-iNScR","outputId":"82d7934b-42fc-4d94-f0b9-f05accd2768c","trusted":true},"cell_type":"code","source":"plt.imshow(masks[1], cmap='gray') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"colab_type":"code","id":"H8nE18MBQIQ-","outputId":"4b484d94-8669-4b7b-d8ed-06006136b769","trusted":true},"cell_type":"code","source":"import numpy.ma as ma\n\ncombined_masks = ma.masked_array(masks[0] > 0, masks[1])\nplt.imshow(combined_masks)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"colab_type":"code","id":"4Qk4WqD3QIT4","outputId":"74faba6d-3ef6-4add-942d-ea76337d79e6","trusted":true},"cell_type":"code","source":"import matplotlib.image as mpimg\nimg=mpimg.imread('../input/caltech256/256_objectcategories/256_ObjectCategories/072.fire-truck/072_0076.jpg')\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Fx2ntaNPV6Or","trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"colab_type":"code","id":"oGLyq3wY1CeO","outputId":"59467051-1495-40fa-c7c4-078f838a9760","trusted":true},"cell_type":"code","source":"mask1 = random_colour_masks(masks[0])\nmask2 = random_colour_masks(masks[1])\nplt.imshow(mask1); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"colab_type":"code","id":"6kH3KPIqWgvg","outputId":"0d41b598-189f-4f19-f7ad-4381d69fa62f","trusted":true},"cell_type":"code","source":"masked1 = cv2.addWeighted(img, 0.5, mask1, 0.5, 0)\nmasked2 = cv2.addWeighted(img, 0.5, mask2, 0.5, 0)\n\nplt.imshow(masked2)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":969},"colab_type":"code","id":"VgThNdv4IlWo","outputId":"5de2ebc4-fd76-44f4-a6ff-7fb3110352e7","trusted":true},"cell_type":"code","source":"!wget https://www.wsha.org/wp-content/uploads/banner-diverse-group-of-people-2.jpg -O mrcnn_standing_people.jpg\ninstance_segmentation_api('./mrcnn_standing_people.jpg', 0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/cricket-image/cric.jpg')","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":720},"colab_type":"code","id":"W_gGkr7KXF4K","outputId":"adbafc5b-596f-4834-d1fa-c89bd907924d","trusted":true},"cell_type":"code","source":"#!wget https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/10best-cars-group-cropped-1542126037.jpg -O mrcnn_cars.jpg\ninstance_segmentation_api('../input/cricket-image/cric.jpg', 0.9, rect_th=5, text_size=5, text_th=5)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":969},"colab_type":"code","id":"K6fUFxTZfx8w","outputId":"4308921a-84e4-43bf-f751-7d0db95b1bba","trusted":true},"cell_type":"code","source":"#!wget https://cdn.pixabay.com/photo/2013/07/05/01/08/traffic-143391_960_720.jpg -O mrcnn_traffic.jpg\ninstance_segmentation_api('../input/caltech256/256_objectcategories/256_ObjectCategories/072.fire-truck/072_0114.jpg', 0.6, rect_th=2, text_size=2, text_th=2)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":852},"colab_type":"code","id":"yaHopudWQuVa","outputId":"f98ed8ff-4ba0-4a7a-e92c-e115d96bf8d5","trusted":true},"cell_type":"code","source":"!http://www.thedetroitbureau.com/wp-content/uploads/2013/05/LA-Traffic.jpg -O mrcnn_birds.jpg\ninstance_segmentation_api('./mrcnn_birds.jpg', 0.9)  ","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":752},"colab_type":"code","id":"7ZhU8JaNTI_t","outputId":"f6f2f368-94e5-4784-d67b-60ad370d9cde","trusted":true},"cell_type":"code","source":"!wget https://www.sciencenews.org/sites/default/files/2018/06/main/articles/062118_BB_koko_feat.jpg -O mrcnn_people.jpg\ninstance_segmentation_api('./mrcnn_people.jpg', 0.8, rect_th=1, text_size=1, text_th=1)    ","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":969},"colab_type":"code","id":"7a5hNqiO5pI2","outputId":"e2ce1f7c-65b9-4b97-9e7b-02f312236373","trusted":false},"cell_type":"code","source":"!wget https://images.unsplash.com/photo-1475505035646-a4680971b0fb -O mrcnn_playing.jpg\ninstance_segmentation_api('./mrcnn_playing.jpg', 0.8, rect_th=6, text_size=6, text_th=6)    ","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"aQwB4oBL5pOD","outputId":"7fdc57e5-501e-4eb5-c198-cc4f32a71005","trusted":false},"cell_type":"code","source":"!wget https://images.unsplash.com/photo-1509205477838-a534e43a849f -O mrcnn_cat_dog.jpg\ninstance_segmentation_api('./mrcnn_cat_dog.jpg', 0.95, rect_th=5, text_size=5, text_th=5)    ","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":922},"colab_type":"code","id":"jUTn2It5GfnJ","outputId":"c067cef6-0899-4481-98a6-27633bf5c513","trusted":false},"cell_type":"code","source":"!wget https://cdn-image.travelandleisure.com/sites/default/files/1556203941/katmai-alaska-bears-AKBEAR0419.jpg -O mrcnn_bear.jpg\ninstance_segmentation_api('./mrcnn_bear.jpg', 0.9, rect_th=6, text_size=5, text_th=5)    ","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"yksDG0dl5oIw","outputId":"696f5c32-233c-49ae-f6fd-1d35c310c3a5","scrolled":true,"trusted":false},"cell_type":"code","source":"!wget https://images.unsplash.com/photo-1505148230895-d9a785a555fa -O mrcnn_elephants.jpg\ninstance_segmentation_api('./mrcnn_elephants.jpg', 0.9, rect_th=6, text_size=6, text_th=6)    ","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":969},"colab_type":"code","id":"ccWkuuZI8_jA","outputId":"274385b4-4f54-4601-d94f-56879949124e","trusted":false},"cell_type":"code","source":"!wget https://images.unsplash.com/photo-1543205764-fee67cf76589 -O mrcnn_baby_teddy.jpg\ninstance_segmentation_api('./mrcnn_baby_teddy.jpg', 0.95, rect_th=6, text_size=6, text_th=6)    ","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"PlbIt2HmYXNs","outputId":"07e706e5-c280-4c50-a073-2e75f256e589","trusted":false},"cell_type":"code","source":"# !wget https://www.westelm.com/weimgs/rk/images/wcm/products/201922/0251/smeg-full-size-refrigerator-o.jpg -O mrcnn_ketchen.jpg\ninstance_segmentation_api('./mrcnn_ketchen.jpg', 0.8, rect_th=2, text_size=1, text_th=1)   ","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"8st2TuBIQkSJ"},"cell_type":"markdown","source":"# Comparing the inference time of model in CPU & GPU\n\n"},{"metadata":{"colab":{},"colab_type":"code","id":"9OYNHMIih4up","trusted":false},"cell_type":"code","source":"import time\n\ndef check_inference_time(image_path, gpu=False):\n  model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n  model.eval()\n  img = Image.open(image_path)\n  transform = T.Compose([T.ToTensor()])\n  img = transform(img)\n  if gpu:\n    model.cuda()\n    img = img.cuda()\n  else:\n    model.cpu()\n    img = img.cpu()\n  start_time = time.time()\n  pred = model([img])\n  end_time = time.time()\n  return end_time-start_time","execution_count":0,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"colab_type":"code","id":"dZoREJXPYhxf","outputId":"55c11bcd-7064-4d2c-d813-d76f10c14a02","trusted":false},"cell_type":"code","source":"cpu_time = sum([check_inference_time('./mrcnn_people.jpg', gpu=False) for _ in range(5)])/5.0\ngpu_time = sum([check_inference_time('./mrcnn_people.jpg', gpu=True) for _ in range(5)])/5.0\n\n\nprint('\\n\\nAverage Time take by the model with GPU = {}s\\nAverage Time take by the model with CPU = {}s'.format(gpu_time, cpu_time))","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"PyTorch_RCNN.ipynb","provenance":[],"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}